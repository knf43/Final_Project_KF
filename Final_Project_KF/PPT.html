<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Lexical prediction mechanisms in Brazilian Portuguese: addressing methodological issues</title>
    <meta charset="utf-8" />
    <script src="libs/header-attrs-2.7/header-attrs.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Lexical prediction mechanisms in Brazilian Portuguese: addressing methodological issues
## Filho &amp; Godoy
### Rutgers University
### April, 29, 2021

---





#Background

Research has found evidence suggesting that specific lexical items and some of their features (e.g., word form and grammatical gender) can be predicted during language processing (e.g., DeLong et al., 2005; Van Berkum et al., 2005). 

In Van Berkum et al. (2005), for example, in a self-paced reading experiment Dutch participants heard stories that supported a prediction of a specific noun.

To investigate whether this noun was anticipated at the preceding indefinite article, stories were continued with a gender-marked adjective whose suffix mismatched the upcoming noun's syntactic gender. 

The results revealed that prediction-inconsistent adjectives slowed readers down before they saw the noun. 
---
# The Study: 

To investigate whether these findings can be replicated in Brazilian Portuguese (BP), Filho &amp; Godoy conducted a self-paced reading experiment examining whether grammatical gender markings in Brazilian Portuguese are used to make predictions during language comprehension. 

###**Example stimuli (Translated into English)**

*The couple looked at the restaurant menu until they could make up their minds. They then called down the waitress, who wrote down...*

+ (predictable) *the*(m) *long*(m) *and detailed* (m) *order*(m) *on her pad.*
+ (unpredictable) *the*(f) *long*(f) *and detailed*(f) *note*(f) *on her pad.*

The authors expected that unpredictable nouns would have a longer reading times. They also expected this effect to show up before the noun, since the preceding words already give away the gender of the noun. 

---
## The analyses

The authors conducted two different analyses: 
+ First, they fit regular linear-mixed models to the words' reading times, which were log-transformed because of their distributions
+ Predictability was the independent variable, and random effects for participants and items were included whenever possible

```r
lmer(rt.log~type + (1+type|subject) + (1+type|item_id))
```
+ Second, they fit generalized models to data in their original, untransformed distribution. The family distribution parameter was set to "gamma."

```r
glmer(rt~type + (1+type|subject) + (1+type|item_id), family = Gamma)
```

---
## Step 1: Loading necessary packages and loading data


```r
library(tidyverse)
library(lme4)
library(lmerTest)
library(readxl)
```

+ The **lme4** package allows one to fit linear and generalized linear mixed-effects models.
+ The **lmerTest** package provides p-values in type I, II or III anova and summary tables for lmer model fits.
+ The **readxl** package makes it easy to get data out of Excel and into R. 
---
## Step 2: Running exclusion criteria
+ Removed data from participants who did the experiment more than once (221 and 222) and who did not complete high school (195 and 237) 


```r
'%ni%' &lt;- Negate('%in%') #declaring function used in the filter
data &lt;- data%&gt;%
    filter(subj %ni% c("221", "222", "195", "237"))
```
+ Removed all data from participants that answered more than 1/4 of the questions incorrectly *n = # of words in sentence*

```r
data &lt;- data%&gt;%
  group_by(subj)%&gt;%
  mutate(accuracy = sum(is_correct == 1)/n())
data &lt;- data%&gt;%
  filter(accuracy &gt;= 0.750)
```
---
## Step 2: Running exclusion criteria
+ Discarded data from observations in which participants answered the questions incorrectly 

```r
data &lt;- data%&gt;%
  filter(is_correct == 1)
```
---
## Step 3: Discarded filler data 

```r
data &lt;- data%&gt;%
    filter(grepl("prev", type))
```
*target data had two types: prev.nao vs. prev.sim*

---
## Step 4: Defined 'item_id', 'subj', 'type' as factors and dropping previously discarded levels before continuing

```r
 data$item_id &lt;- as.factor(data$item_id)
  data$subj &lt;- as.factor(data$subj)
  data$type &lt;- as.factor(data$type)
  data$subj &lt;- droplevels(data$subj)
  data$subj_uid &lt;- droplevels(data$subj_uid)
```
+ 'item_id', 'subj', 'type' were defined as as factors so that R would recognize them as factors (tells you how many groups) rather than characters (which are read as strings)

+ It's a way of making sure R reads them as categorical variables.  
---
## Step 5: Loaded the critical regions for each item from an excel file

```r
regions &lt;- read.csv("./Project_Docs/critical_regions.csv")
```
---
## Step 6: Added columns with the critical regions to the main data frame 

```r
regions$item_id &lt;- as.factor(regions$item_id)

data &lt;- left_join(data, regions, by = "item_id")
  
cloze &lt;- read_excel("cloze.xlsx")
cloze$item_id &lt;- as.factor(cloze$item_id)
cloze$type &lt;- as.factor(cloze$type)
  
data &lt;- left_join(data, cloze, by = c("item_id", "type"))
```
---
## Step 7: Organized data by regions of interest 

```r
rt_art &lt;- data%&gt;%
  filter(region == art)

rt_adj1 &lt;- data%&gt;%
  filter(region == adj1)

rt_conj &lt;- data%&gt;%
  filter(region == conj)

rt_adj2 &lt;- data%&gt;%
  filter(region == adj2)

rt_subst &lt;- data%&gt;%
  filter(region == subst)

rt_spill &lt;- data%&gt;%
  filter(region == spill)
```
---
## Step 7: Organized data by regions of interest 

Created 6 subsets out of each of the regions with the RTs for each region. 

Self-paced reading forces participants to read sentences sequentially, with no looking ahead or looking back; however, participants can continue processing a word as they look at later words. 

This can lead to “spillover” effects, where the difficulty induced by a given word slows RTs one or more words further downstream and may not show up on the word in question (e.g., Koornneef &amp; van Berkum, 2006; Smith and Levy, 2013). 

To account for this for this, self-paced reading is often analyzed using a multi-word spillover region
---
## Step 8: Checking data distribution
The authors checked data distribution for each of the regions using: 
+ an overall histogram
+ a histogram broken up by type
+ a box plot broken up by type

For example...
---

```r
ggplot(rt_art, aes(x = rt))+
  geom_histogram()
```

![](./Analysis_Docs/box1)&lt;!-- --&gt;
---

```r
ggplot(rt_art, aes(x = rt))+
  geom_histogram()+
  facet_wrap(~type)
```

![](./Analysis_Docs/box2)&lt;!-- --&gt;
---

```r
ggplot(rt_art, aes(x = type, y = rt))+
  geom_boxplot()
```

![](./Analysis_Docs/plot3)&lt;!-- --&gt;
---
## Step 9: Eliminating outliers (any observation &gt; 2500 ms)

```r
rt_art &lt;- rt_art%&gt;%
  filter(rt &lt;= 2500 &amp; rt &gt;= 100) 

rt_adj1 &lt;- rt_adj1%&gt;%
  filter(rt &lt;= 2500 &amp; rt &gt;= 100) 

rt_conj &lt;- rt_conj%&gt;%
  filter(rt &lt;= 2500 &amp; rt &gt;= 100) 

rt_adj2 &lt;- rt_adj2%&gt;%
  filter(rt &lt;= 2500 &amp; rt &gt;= 100) 

rt_subst &lt;- rt_subst%&gt;%
  filter(rt &lt;= 2500 &amp; rt &gt;= 100) 

rt_spill &lt;- rt_spill%&gt;%
  filter(rt &lt;= 2500 &amp; rt &gt;= 100) 
```
---
## Step 10: check distributions again 

```r
ggplot(rt_art, aes(x = rt))+
  geom_histogram()
```

![](./Analysis_Docs/filter1)&lt;!-- --&gt;
---

```r
ggplot(rt_art, aes(x = rt))+
  geom_histogram()+
  facet_wrap(~type)
```

![](./Analysis_Docs/filter2)&lt;!-- --&gt;
---

```r
ggplot(rt_art, aes(x = type, y = rt))+
  geom_boxplot()
```

![](./Analysis_Docs/filter3)&lt;!-- --&gt;
---
## Step 11: Performing log transformations on the data 
Visual inspection showed that data followed a non-normal distribution

A log transformation can be used to make highly skewed distributions less skewed (Decreases the difference between values)
---

```r
ggplot(rt_art, aes(x = rt.log))+
  geom_histogram()
```

![](./Analysis_Docs/log_histogram_1)&lt;!-- --&gt;
---

```r
ggplot(rt_art, aes(x = type, y = rt.log))+
  geom_boxplot()  
```

![](./Analysis_Docs/log_2)&lt;!-- --&gt;
---
## Step 12: Descriptive statistics (before and after outlier removal)


```r
rt_art%&gt;%
  group_by(type)%&gt;%
  summarise(mean = mean(rt), median = median(rt)) 
```
Before: 
+ **Type 1** prev.nao  404 (mean).    372 (median)
+ **Type 2** prev.sim  400 (mean).    374 (median)

After: 
+ **Type 1** prev.nao  401 (mean).    372 (median)
+ **Type 2** prev.sim  398 (mean).    374 (median)

When data has a normal distribution the mean and median will be close together 
---
## Step 13: Adjusting models
Models that have been commented out are the ones that failed to converge or that were not needed. 

```r
#model.art &lt;- lmer(rt.log ~ type + (1+type|subj) + (1+type|item_id), data = rt_art, REML = FALSE)
#model.art &lt;- lmer(rt.log ~ type + (1+type|subj) + (1|item_id), data = rt_art, REML = FALSE)
model.art &lt;- lmer(rt.log ~ type + (1|subj) + (1|item_id), data = rt_art, REML = FALSE
```


```r
summary(model.art)
```
---

```r
Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite's method [lmerModLmerTest]
Formula: rt.log ~ type + (1 | subj) + (1 | item_id)
   Data: rt_art

     AIC      BIC   logLik deviance df.resid 
  1396.3   1430.1   -693.2   1386.3     6370 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.9228 -0.5858 -0.1146  0.4583  8.2100 

Random effects:
 Groups   Name        Variance Std.Dev.
 subj     (Intercept) 0.041009 0.20251 
 item_id  (Intercept) 0.001846 0.04296 
 Residual             0.063028 0.25105 
Number of obs: 6375, groups:  subj, 337; item_id, 20

Fixed effects:
               Estimate Std. Error         df t value Pr(&gt;|t|)    
(Intercept)   5.934e+00  1.530e-02  9.360e+01 387.865   &lt;2e-16 ***
typeprev.sim -3.299e-03  6.373e-03  6.040e+03  -0.518    0.605    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Correlation of Fixed Effects:
            (Intr)
typeprev.sm -0.208
```

---
#References

DeLong, K., Urbach, T. &amp; Kutas, M. Probabilistic word pre-activation during language comprehension inferred from electrical brain activity. *Nature Neuroscience, 8*, 1117–1121 (2005). 

Koornneef, A. W. and van Berkum, J. J. (2006). On the use of verb-based implicit causality in sentence comprehension : Evidence from self-paced reading and eye tracking. *Journal of Memory and Language, 54*(4), 445–465.

Smith, N. J. and Levy, R. (2013). The effect of word predictability on reading time is logarithmic. *Cognition, 128*(3), 302–319.

Van Berkum, J. J., Brown, C. M., Zwitserlood, P., Kooijman, V., &amp; Hagoort, P. (2005). Anticipating upcoming words in discourse: evidence from ERPs and reading times. *Journal of experimental psychology. Learning, memory, and cognition, 31*(3), 443–467.

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
